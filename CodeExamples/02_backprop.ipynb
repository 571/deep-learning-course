{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 598 Backpropagation, optimization and regularization tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation tutorial is adapted from ( https://github.com/jcjohnson/pytorch-examples )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are powerful models that can approximate any function [(theorem)](https://en.wikipedia.org/wiki/Universal_approximation_theorem). By forward propagating an input through a neural network, we can extract information necessary to map that input to a desired output (e.g. class label). However, we need to adjust the network connections based on input/output pairs seen in training examples. This process can be achieved by the backpropagation algorithm (Backprop) and stochastic gradient descent (SGD).\n",
    "\n",
    "This tutorial assumes understanding basic linear algebra operations such as dot product, and also assumes understanding of derivatives and the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will cover:\n",
    "\n",
    "* Forward propagation\n",
    "* Backpropagation\n",
    "* Optimization: SGD with Momentum, AdaGrad, RMSProp, Adam\n",
    "* Regularization: L1, L2, Dropout, and Early Stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use pytorch throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML, Image\n",
    "from IPython.display import YouTubeVideo\n",
    "from sympy import init_printing, Matrix, symbols, Rational\n",
    "import sympy as sym\n",
    "from warnings import filterwarnings\n",
    "init_printing(use_latex = 'mathjax')\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks consist of many layers of computation that start from the lowest level (i.e. network input). Each layer consists of a dot product operation on the input followed by adding a bias term and an activation function.\n",
    "Note: We do not add bias term in this tutorial, however, this is left for students as an exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single layer computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create some dummy data points\"\"\"\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension\n",
    "N, D_in, H = 64, 1000, 100\n",
    "x = torch.randn(N, D_in, device=device)     # Input to network (layer0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create weight for the single layer, and forward through it with a ReLU activation\"\"\"\n",
    "w1 = torch.randn(D_in, H, device=device)  # First layer weight matrix\n",
    "b1 = torch.zeros(1, H, device=device)        # First layer bias\n",
    "a1 = x.mm(w1) + b1                        # Preactivation of layer 1\n",
    "h1 = a1.clamp(min=0)                      # Layer 1 output (ReLU activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create weight matrix for the next layer layer and forward through it taking the previous layer output as input\"\"\"\n",
    "D_out = 10  \n",
    "w2 = torch.randn(H, D_out, device=device)   # Second layer weight matrix\n",
    "b2 = torch.zeros(1, D_out, device=device)   # Second layer bias\n",
    "yhat = h1.mm(w2) + b2                       # Linear activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Forward propagate through a single layer network and backpropagate for max_steps steps\"\"\"\n",
    "\n",
    "# Create random input/output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, D_out, device=device)\n",
    "b1 = torch.zeros(1, D_out, device=device)\n",
    "\n",
    "# Define optimization (learning) steps\n",
    "max_steps = 500\n",
    "\n",
    "# Define learning rate\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# Container to store loss through training\n",
    "loss_list = [] \n",
    "for t in range(max_steps):\n",
    "    # Forward pass: compute predicted y (yhat)\n",
    "    a1 = x.mm(w1) + b1                      # Layer 1 pre-activation\n",
    "    yhat = a1.clone()                       # Layer 2 activation (linear activation)\n",
    "    \n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = (yhat - y).pow(2).sum()\n",
    "    loss_list.append(loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 with respect to the squared loss\n",
    "    # Gradient of the loss layer with respect to the network output\n",
    "    grad_yhat = 2.0 * (yhat - y)            # dC_dyhat\n",
    "    \n",
    "    # Gradients in layer 1 take in gradients of the cost with respect to its output\n",
    "    # and computes its internal gradients. Note: activation is ReLU!!\n",
    "    grad_a1 = grad_yhat.clone()             # dC_da1 -> dC_dyhat * dyhat_da1 Gradient of Linear activation\n",
    "    grad_w1 = x.t().mm(grad_a1)             # dC_dw1 = da1_dw1 \\dot dC_da1\n",
    "    grad_b1 = torch.ones(1, N).mm(grad_a1)  # dC_db1 = da1_db1 \\dot dC_da1\n",
    "    grad_x = grad_a1.mm(w1.t())             # dC_dx = dC_da1 \\dot da1_dx\n",
    "\n",
    "    # No more layers! Let's update the parameters using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    b1 -= learning_rate * grad_b1\n",
    "\n",
    "\n",
    "# Plot learning curve\n",
    "plt.title('Learning curve.')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Optimization steps.')\n",
    "plt.plot(loss_list, '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement a two layer network backward pass with ReLU activation in the first hidden layer (15 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Forward propagate through network and backpropagate for max_steps steps\"\"\"\n",
    "\n",
    "# Create random input/output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "b1 = torch.zeros(1, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "b2 = torch.zeros(1, D_out, device=device)\n",
    "\n",
    "# Define optimization (learning) steps\n",
    "max_steps = 500\n",
    "\n",
    "# Define learning rate\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# Container to store loss through training\n",
    "loss_list = [] \n",
    "for t in range(max_steps):\n",
    "    # Forward pass: compute predicted y (yhat)\n",
    "    a1 = x.mm(w1) + b1              # Layer 1 preactivation \n",
    "    h1 = a1.clamp(min=0)            # Layer 1 output\n",
    "    a2 = h1.mm(w2) + b2             # Layer 2 pre activation\n",
    "    yhat = a2.clone()               # Layer 2 activation (linear activation)\n",
    "    \n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = (yhat - y).pow(2).sum()\n",
    "    loss_list.append(loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    # Gradient of the loss layer with respect to input\n",
    "    grad_yhat = 2.0 * (yhat - y)            # dC_dyhat\n",
    "    \n",
    "    \n",
    "    # Gradients in layer 2 take in gradient of the cost with respect of its output\n",
    "    # and computes its internal gradients. Note: activation is linear so\n",
    "    grad_a2 = grad_yhat.clone()             # dC_da2 -> dC_yhat * dyhat_a2 Gradient of linear activation\n",
    "    grad_w2 = h1.t().mm(grad_a2)            # dC_dw2 = da2_dw2 \\dot dC_da2\n",
    "    grad_b2 = torch.ones(1, N).mm(grad_a2)  # dC_db2 = dyhat_db2 \\dot dC_da2\n",
    "    grad_h1 = grad_a2.mm(w2.t())            # dC_dh1 = dC_da2 \\dot da2_dh1\n",
    "\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\" COMPLETE CODE BELOW \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    # Gradients in layer 1 take in gradients of the cost with respect to its output\n",
    "    # and computes its internal gradients. Note: activation is ReLU!!\n",
    "#     grad_a1 = ...                           # dC_da1 = dC_dh1 * dh1_da1 Gradient of ReLU activation\n",
    "#     grad_w1 = ...                           # dC_dw1 = da1_dw1 \\dot dC_da1\n",
    "#     grad_b1 = ...                           # dC_db2 = da1_db1 \\dot dC_da1\n",
    "#     grad_x = ...                            # dC_dx = dC_da1 \\dot da1_dx\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    # No more layers! Let's update the parameters using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    b1 -= learning_rate * grad_b1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    b2 -= learning_rate * grad_b2\n",
    "\n",
    "\n",
    "# Plot learning curve\n",
    "plt.title('Learning curve.')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Optimization steps.')\n",
    "plt.plot(loss_list, '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement the model training in PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Forward propagate through network and backpropagate for max_steps steps with PyTorch modules\"\"\"\n",
    "# Define optimization (learning) steps\n",
    "max_steps = 500\n",
    "\n",
    "# Define learning rate\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# Container to store loss through training\n",
    "loss_list = []\n",
    "\n",
    "# Define network with similar initialization as the above example for consistency\n",
    "# Layer1\n",
    "layer1 = torch.nn.Linear(D_in, H)\n",
    "layer1.weight = torch.nn.Parameter(torch.randn(H, D_in, device=device))\n",
    "layer1.bias = torch.nn.Parameter(torch.zeros(H, device=device))\n",
    "\n",
    "# Layer2\n",
    "layer2 = torch.nn.Linear(H, D_out)\n",
    "layer2.weight = torch.nn.Parameter(torch.randn(D_out, H, device=device))\n",
    "layer2.bias = torch.nn.Parameter(torch.zeros(D_out, device=device))\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    layer1,\n",
    "    torch.nn.ReLU(),\n",
    "    layer2\n",
    ").to(device)\n",
    "\n",
    "# Initialize parameters similar to the example above\n",
    "for param in net.parameters():\n",
    "    if len(param.shape) > 1:\n",
    "        param = torch.nn.Parameter(\n",
    "            torch.randn(param.shape[0], param.shape[1], device=device))\n",
    "    else:\n",
    "        param = torch.nn.Parameter(torch.zeros_like(param))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), learning_rate)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "for t in range(max_steps):\n",
    "    # Forward pass and loss computation\n",
    "    yhat = net.forward(x)\n",
    "    loss = loss_fn(yhat, y)\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "    # Compute gradients using backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Take gradient step\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# Plot learning curve\n",
    "plt.title('Learning curve.')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Optimization steps.')\n",
    "plt.plot(loss_list, '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a vast variety of optimizers that we can use to optimize our network parameters. We will focus on the following: SGD, SGD with Momentum, AdaGrad, Adadelta, RMSProp and Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent (SGD) is simply a parameter update algorithm that maximizes an objective function:\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\nabla_{\\theta_t} J(\\theta_t), \\nonumber\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\ell$ is the objective function we want to optimize, $\\theta_t$ and $\\theta_{t+1}$ are the parameters in the network before and after the update, respectively. Finally, $\\alpha$ is the step size of the update (learning rate).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD with Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD with momentum, in contrast to simple SGD, is a method that prevents oscillations during an update by adding the scaled previous update in the optimization to the current update:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{t+1} =& \\gamma v_{t} + \\alpha \\nabla_{\\theta_t} J(\\theta_t), \\nonumber \\\\\n",
    "\\theta_{t+1} =& \\theta_t + v_{t+1}, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $v_0$ is initialized to zeros in the first optimization of the timestep.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad or adaptive gradient algorithm is a parameter update method that adapts the learning rate based on whether the parameters represent frequently ocurring features or infrecuently ocurring features.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g_{t} =& \\nabla_{\\theta_{t}} J(\\theta_{t}), \\nonumber \\\\\n",
    "\\theta_{t+1} =& \\theta_{t} - \\frac{\\alpha}{\\sqrt{G_{t} + \\epsilon}} \\odot g_{t}, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $G_{t} \\in \\mathbb{R}^{d \\times d}$ is a diagonal matrix where $G_{i,i}$ is $\\sum^{t}_1 g_{t,i}^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp is similar to adagrad but with one difference: Instead of accumulating all past squared gradients, it looks at a window of previous gradients defined by a decaying factor.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[g^2]_{t} =& \\gamma \\mathbb{E}[g^2]_{t-1} + (1 - \\gamma)g_{t}^2, \\nonumber \\\\\n",
    "\\theta_{t+1} =& \\theta_{t} - \\frac{\\alpha}{\\sqrt{\\mathbb{E}[g^2]_{t} + \\epsilon}} \\odot g_{t}, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adadelta was developed around the same time as RMSProp. This update rule, however, completely remove the learning rate from the equation by a running average of the squared parameter updates in the numerator:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[g^2]_{t} =& \\rho \\mathbb{E}[g^2]_{t-1} + (1 - \\rho)g_{t}^2, \\nonumber \\\\\n",
    "\\mathbb{E}[\\nabla \\theta^2]_{t-1} =& \\rho \\mathbb{E}[\\nabla \\theta^2]_{t-2} + (1 - \\rho)\\nabla \\theta_{t}^2, \\nonumber \\\\\n",
    "\\nabla \\theta_t =& -\\frac{\\sqrt{\\mathbb{E}[\\nabla \\theta^2]_{t-1} + \\epsilon}}{\\sqrt{\\mathbb{E}[g^2]_{t} + \\epsilon}} \\odot g_{t}, \\nonumber \\\\\n",
    "\\theta_{t+1} =& \\theta_{t} + \\nabla \\theta_t, \\nonumber\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam or adaptive moment estimation is another adaptive learning rate method with respect to each parameter. This method also keeps an exponentially decaying average of past gradients. However, in constrast to previous methods, a momentum like term is used which keeps an exponentially decaying average of past gradients. The parameter update is performed as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m_{t} =& \\beta_1 m_{t-1} + (1 - \\beta_1)g_{t}, \\nonumber \\\\\n",
    "v_{t} =& \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2, \\nonumber \\\\\n",
    "\\hat{m}_t =& \\frac{m_t}{1-\\beta_1^t}, \\nonumber \\\\\n",
    "\\hat{v}_t =& \\frac{v_t}{1-\\beta_2^t}, \\nonumber \\\\\n",
    "\\theta_{t+1} =& \\theta_{t} - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon}\\hat{m}_t, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $m_t$ and $v_t$ are the first and second moment estimates of the gradients while $\\hat{m}_t$ and $\\hat{v}_t$ are bias corrected values of the two moments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a rough intuition of what the previously mentioned optimizers are doing, we can run some network trainings and try each one of them! Take your time to play with the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = []\n",
    "\n",
    "# Remove optimizers by commenting lines below\n",
    "# Try different hyperparameters to see any behavior change\n",
    "optimizers.append(\n",
    "    lambda params: torch.optim.SGD(\n",
    "        params, lr=1e-4))\n",
    "optimizers.append(\n",
    "    lambda params: torch.optim.SGD(\n",
    "        params, lr=1e-4, momentum=0.9))\n",
    "optimizers.append(\n",
    "    lambda params: torch.optim.Adagrad(\n",
    "        params, lr=1e-2))\n",
    "optimizers.append(\n",
    "    lambda params: torch.optim.RMSprop(\n",
    "        params, lr=1e-4, alpha=0.99, eps=1e-08))\n",
    "optimizers.append(\n",
    "    lambda params: torch.optim.Adadelta(\n",
    "        params, rho=0.9, lr=1e-2, eps=1e-06))\n",
    "optimizers.append(\n",
    "    lambda params: torch.optim.Adam(\n",
    "        params, lr=1e-4, eps=1e-08, betas=(0.9, 0.999)))\n",
    "\n",
    "losses_list = []\n",
    "optimizer_name = []\n",
    "for optim in optimizers:\n",
    "    \"\"\"Forward propagate through network and backpropagate\n",
    "    for max_steps steps with PyTorch modules\"\"\"\n",
    "    # Define optimization (learning) steps\n",
    "    max_steps = 500\n",
    "\n",
    "    # Define learning rate\n",
    "    learning_rate = 1e-6\n",
    "\n",
    "    # Container to store loss through training\n",
    "    closs_list = []\n",
    "\n",
    "    # Build network\n",
    "    net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(D_in, H),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(H, D_out)\n",
    "    ).to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim(net.parameters())\n",
    "\n",
    "    # Define loss function\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Forward pass and loss computation\n",
    "        yhat = net.forward(x)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        closs_list.append(loss.item())\n",
    "\n",
    "        # Compute gradients using backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Take gradient step\n",
    "        optimizer.step()\n",
    "\n",
    "    losses_list.append(closs_list)\n",
    "    optimizer_name.append(str(optimizer).split(' (')[0])\n",
    "    if (optimizer_name[-1] == 'SGD' and\n",
    "        ('momentum', 0.9) in optimizer.defaults.items()):\n",
    "        optimizer_name[-1] += ' with Momentum'\n",
    "\n",
    "for closs_list, name in zip(losses_list, optimizer_name):\n",
    "    # Plot learning curve\n",
    "    plt.title('Learning curve.')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('Optimization steps.')\n",
    "    plt.plot(closs_list, '--', label=name)\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement a four layer neural network and optimize it as described below (10 minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random input/output data\n",
    "D_in = 500\n",
    "D_out = 20\n",
    "N = 64\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Define optimization (learning) steps\n",
    "max_steps = 500\n",
    "losses_list = []\n",
    "\n",
    "# Define learning rate\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# Container to store loss through training\n",
    "closs_list = []\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\" COMPLETE CODE BELOW \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# Build network with the following description:\n",
    "#     layer 1: hidden_units=100, activation=Sigmoid\n",
    "#     layer 2: hidden_units=200, activation=Tanh\n",
    "#     layer 3: hidden_units=100, activation=ReLU\n",
    "# net = ...\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\" COMPLETE CODE BELOW \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# Use Adam optimizer for network optimization\n",
    "# optimizer = ...\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "for t in range(max_steps):\n",
    "    # Forward pass and loss computation\n",
    "    yhat = net.forward(x)\n",
    "    loss = loss_fn(yhat, y)\n",
    "    closs_list.append(loss.item())\n",
    "\n",
    "    # Compute gradients using backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Take gradient step\n",
    "    optimizer.step()\n",
    "\n",
    "# Plot learning curve\n",
    "plt.title('Learning curve.')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Optimization steps.')\n",
    "plt.plot(closs_list, '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are powerful devices. However, if data is small and we are not careful, they will try to fit the training data as perfectly as possible ignoring the true underlying data structure. More importantly, if any noise is present, they will end up overfitting to the noise itself!\n",
    "Let us create some data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some data and plot it\n",
    "# Train data\n",
    "xtrain = torch.FloatTensor([i * 3.14159 / 180 for i in range(0, 200, 4)])\n",
    "noise = torch.normal(\n",
    "    torch.zeros(xtrain.shape[0]), torch.ones(xtrain.shape[0]) * 0.2)\n",
    "ytrue = torch.sin(xtrain)\n",
    "ynoise_train = torch.sin(xtrain) + noise\n",
    "\n",
    "# Validation data\n",
    "xval = torch.FloatTensor([i * 3.14159 / 180 for i in range(2, 200, 4)])\n",
    "noise = torch.normal(\n",
    "    torch.zeros(xval.shape[0]), torch.ones(xval.shape[0]) * 0.2)\n",
    "ynoise_val = torch.sin(xval) + noise\n",
    "\n",
    "plt.plot(xtrain.numpy(), ytrue.numpy(), '--', label='true data')\n",
    "plt.plot(xtrain.numpy(), ynoise_train.numpy(), '.', label='noisy data')\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to regularize neural networks. The first are to purposedly try to reduce the network capacity. In essense, just drop as many weight connections as neccessary. In this tutorial, we will experiment with $\\textbf{L1}$ and $\\textbf{L2}$ weight regularization:\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "J(\\theta) = \\ell(f(x), y; \\theta) + \\lambda \\mathcal{R}(\\theta),\\nonumber\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\ell(f(x), y; \\theta)$ is the loss between the network output and the groundtruth data, $\\mathcal{R}$ is a regularizer function, and $\\lambda$ is a weight we can choose to give the regularization. For L2 regularization $\\mathcal{R}(\\theta) = \\sum^k \\|\\theta_k\\|^2_2$, and for L1 we use $\\mathcal{R}(\\theta) = \\sum^k |\\theta_k|$.\n",
    "\n",
    "Other types of regularization include the highly popular $\\textbf{Dropout}$. In essence, dropout turns off random neurons in the network randomly which results in random smaller subset of neurons being able to predict the data. When we apply weight decay, we learn a single \"smaller\" network that predicts the data. In dropout, we essentially learn an average of multiple smaller networks that predict the data which collaboratively perform better than the single large network with which we started.\n",
    "\n",
    "Finally, another way to prevent overfitting is $\\textbf{early stopping}$. For this technique, we need validation data which we use to test the currently learned network during training. If the validation error starts going down, this indicates overfitting is happening. Once this happens, we stop the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training steps\n",
    "max_steps = 10000\n",
    "hidden_units = 50\n",
    "lambda_ = 0.0001\n",
    "\n",
    "# For L1 or L2 regularization\n",
    "# Uncomment either l1 or l2 to be applied to the network training\n",
    "reg_type = None\n",
    "# reg_type = 'l1'\n",
    "# reg_type = 'l2'\n",
    "\n",
    "# For dropout regularization\n",
    "p = 0.0 # percentage of units to drop out [0, 1]\n",
    "\n",
    "# For early stopping\n",
    "early_stopping = False\n",
    "prev_val_loss = None\n",
    "\n",
    "# Now, let's build a four layer network and train\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, hidden_units),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=p),\n",
    "    torch.nn.Linear(hidden_units, hidden_units),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=p),\n",
    "    torch.nn.Linear(hidden_units, hidden_units),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=p),\n",
    "    torch.nn.Linear(hidden_units, 1)\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adagrad(net.parameters(), lr=1e-2)\n",
    "\n",
    "# Define loss\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Train\n",
    "for t in range(max_steps):\n",
    "    # Set network to training mode and feedforward through it\n",
    "    net.train()\n",
    "    yhat = net.forward(xtrain[:, None])\n",
    "    \n",
    "    # Choose type of weight regularizer\n",
    "    reg_loss = 0.0\n",
    "    if reg_type is not None:\n",
    "        if reg_type == 'l1':\n",
    "            regularizer = lambda x: torch.abs(x)\n",
    "        elif reg_type == 'l2':\n",
    "            regularizer = lambda x: x ** 2\n",
    "        else:\n",
    "            raise Exception('Unknown regularization: %d' %reg_type)\n",
    "\n",
    "        for param in net.parameters():\n",
    "            reg_loss += regularizer(param).sum()\n",
    "        \n",
    "    \n",
    "    # Loss computation\n",
    "    loss = loss_fn(yhat, ynoise_train[:, None]) + lambda_ * reg_loss\n",
    "    \n",
    "    # Compute gradients using backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Take gradient step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Check early stopping criteria\n",
    "    if early_stopping and t % 100 == 0:\n",
    "        net.eval()\n",
    "        yhat_val = net.forward(xval[:, None])\n",
    "        curr_val_loss = loss_fn(yhat_val, ynoise_val[:, None])\n",
    "        if prev_val_loss is not None and prev_val_loss > curr_val_loss:\n",
    "            print('Early stopping activated!!')\n",
    "            print('Previous validataion loss %f.' %prev_val_loss)\n",
    "            print('Current validataion loss %f.' %curr_val_loss)\n",
    "            break\n",
    "        prev_val_loss = curr_val_loss\n",
    "\n",
    "# Eval mode to turn off dropout\n",
    "net.eval()\n",
    "yhat = net.forward(xtrain[:, None])\n",
    "    \n",
    "plt.plot(xtrain.numpy(), ynoise_train.detach().numpy(), '.', label='noisy data')    \n",
    "plt.plot(xtrain.numpy(), yhat.detach().numpy(), '--', label='predictions')\n",
    "plt.plot(xtrain.numpy(), ytrue.detach().numpy(), '--', label='true data')\n",
    "plt.legend(loc='lower left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement a neural network for binary classification. (until the end of class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Make use of any of the techniques provided in this tutorial to optimize your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJztnX+IZcl137+n3/TTurtlhN4uwfJuv3FAiGyEokiDkbExxBuHzVpI2InBs71CqxgNedjDhiQILwMKxixJEASZVUCeyEKKbuMQiIUTWclKwgqegG25N1opK+tHFO/8kGMyOytHmdmZlWe7K3/cV93V9erHqXvr/uzzgWKm37s/6t5377dOnTp1ipRSEARBEMbDWtcVEARBEPIiwi4IgjAyRNgFQRBGhgi7IAjCyBBhFwRBGBki7IIgCCNDhF0QBGFkiLALgiCMDBF2QRCEkXGqi5Pee++96vTp012cWhAEYbA8++yzN5RS98W260TYT58+jb29vS5OLQiCMFiI6ApnO3HFCIIgjAwRdkEQhJEhwi4IgjAyRNgFQRBGhgi7IAjCyBBhFwRBGBki7IIgCCNDhF0QBGFkiLALQtfs7gKnTwNra+W/u7td10gYOCLswjgZilju7gLnzgFXrgBKlf+eO9ff+gqDoJOUAoLQKFosb98u/9ZiCQA7O93Vy8WFC0f11Ny+Dbz3veX/+1ZfYRCIxS6MD59YXrjQTX1CXL3q/nx/Xyx3oTIi7ML48Iml7/Mu2d72f9fXxkjoPSLsQv+o6x/3iWVIRLviqaeAjQ3/931sjITeI8Iu9Iscg4kusdzYKD/vGzs7wMWLwGTi/r6PjZHQe7IJOxFNiOjLRPSZXMcUTiA5/ONaLOdzgKj89+LF/g5E7uwAn/zkcBojoffkjIp5AsDXAfxgxmMKJ41c/vGdnf4KuQtd1wsXymvd3i5FfUjXIPSGLBY7Ed0P4GcAfCzH8YQTzJD847nZ2QEuXwYODsp/RdSFiuRyxXwYwAcAHGQ6nnBSyeUfH8oEJUFogNrCTkTvBHBdKfVsZLtzRLRHRHsvvvhi3dMKYyWHf1xmc/YDaVw7g5RS9Q5A9M8BvAfAqwDuQelj/22l1GO+fc6cOaNkMWuhMU6fLsXcZj4vXRxC89izf4Gy59XnQewBQETPKqXOxLarbbErpZ5USt2vlDoN4BcA/F5I1AWhcYY0QakKQ7CEfdFNTzzR/7qPAIljF8ZHnQHYvotmzM3Ul/r7GtGXXhIXWRsopVovb3/725UgNEZRKLWxoVQpH2XZ2Cg/b2K/NpnPj9dPl/m8X/X31dNXd4EFgD3F0Fix2IXxUXUAti/Jw0JWd8jNVLf+Oa39WKoEk7G4yPoER/1zF7HYhV7isyiJ2qtDzOoOWexE1evfhLVfFEf1ms+Vms3EYq8JxGIXTjSp1ufubmndu2hzclTM6g7F+dcZW2iit2JPuPr1X5e0CW3BUf/cRSx2oVGqWJ8+S5gor9UaOxbH6vYds47VXcfaTyH1fgjHANNiF2EXxkfIXeHDJ2xA9XrkbGC47oqqwln3vEIrcIVdXDHC+KgSx+5zV8zn1etRxb1RN6VC1XwzQ0p1LEQRYRfGRxVfcxPCVqWBqZtSwTe2EBtzGFqqYyEMx6zPXcQVIzRKnTj2nP7ftt0bvuteLPoT3y7UAuJjF1qlb4NiVeuT8zranjDkCyecTMINTN9+O8GLCLvQHn2a8ZiKFjVAqbW1VfHLHcvd1D0pCrd4h4qO+Bnqb3cCEWEX2mOoERUuUWvyOnL3BjiTf2IW+1B/uxMKV9hl8FSoz1CzKbqiVlzo69ADkETAqVPlv/feWxbzM3twcne33Oaxx6onwDIHP++9F3jf+44f66WX/PueO+cfGB7qbyeE4ah/7iIW+8gYktVnWrpcl4UvwVaoaHdGbD/OPUo9t1lms9XrNnsLQ/rtBHHFCC0yFD9tFYHU15GSrTDmArH93DGqnBtQan29FPaQ62cov52glBJhF9pmCJEVqQI5mx1dR4qFn1L0vQrdO+65NzePjjObKTWd8gSb89sN4fc9AYiwC4JNTCC1hT2brVq6Va3mWG/AFWNuNyrcc0+nzbhYYla9S/SlIWgEEXZBsOGI3WKx2gCEBLhq0cIdEm2un951LTmTeqUu7jGdroaOrq+LuGdAhF0QbDiWp08Q5/NS3H0uENPiD/nWbeuV04vQrhWzF+HbXgt3Tos91Eik9GT0QK5QGa6w1w53JKJ7iOhLRPQVIvoaEf1q3WMKQiPE8qFcuFBKkIsrV4Df+A33d/pYBwflv77wwqJYTcwVy5W+v1/W6aWXgDt3gE99qgxTnEzc2ytV1ufq1TL80q5Dldw3odw7KWGRoZBMIS8c9Q8VAARga/n/dQB/BOAdoX3EYhd6Sa4BUu264fiYUyN1ZrO07be26vu5Qz2d1LEHoRZoy2Jfnu/W8s/1ZfGYPcKJJueamk2Qa6Wk27eBz36Wlz5X9yJmM96xX3qJN6lKc+dOegpfm1BPJ2VtU+419pW+P78mHPWPFQATAM8BuAXgX8a2F4v9BDKEeOk6E4F8vu7U82srnxMD3xcrmWO5mxE7Q6Qnzy+6GDwF8DoAXwTwZsd35wDsAdjb3t5u/g4I/aKvMxztsDzbhVJVTEPXZQqhFnDbVeITklBOmFAj03TIYWjgeTIZtqgr1ZvntxNhL8+LDwL4p6FtxGI/gYQiK7qKeeZYYTFxf+ihNEsu1Cuw9yuK40I+m5Xnc+3rykwZO35OOJE6Q6atNWEjtCbsAO4D8Lrl/38AwCUA7wztI8J+AvG9+K7BwLa6uBwrjJPrJaVhijUUKee2y9ZW3IXjsjBzNKyhgeeue2U5OGkWO4C3APgygK8CeB7AB2P7iLCfQFJdC228MCExWiyO1z2XNRqLvDGPl+oKMveNWZimO8g1IStV3H111T2yVPo2c/Uk+9i5RYT9hOJ6Wbvs4saE0xT3XBZbisWeGn5p+rJTZ4vWvS7XMYmO30N7e59wNyGiORqKHjQ2IuzCMOiyixsTuMkkvK1LbGIvf4qPvcrgbSgNQUrseahh9V0jV/hi9zJ3nhtXr7BvEVlMRNiFYdBGFzdmHYYEjnuclGvhRMX4jscpWgB950k5hute1v29YsKdqxfHGR8ZGCLswnBosovLESLfgKNpsXPOE1s0umr9feGYHGvb5yLhWP0ucljTMeFuy+01wGgdEXZhMDTquuRmdHRt4/MPuy4gZBk2JSCcawsNarr+jv0AIaHk4hsw10nCfBk2Ux+MWAMmFrsIu9AMlXv23NaA261fLI5nZ+SKulL8AdEcvmnbeo/dvFgYYkprGhrs1sLMOV5I2FMHYUNwUiIPDBF2YRBU6nXHXn7O1PyY2KYQswwXC38L9tBDPOvUt789U9b+W6cU9lnHIex74ztWqnBWSQNcdeA0tojJwBBhF9zUFLLcbpNK42Qh9wJnQQy9XVGUC0CY33EXhOA0HqYopUa42EKWYzJVSNjtHotrRm1qmc3csfK+WbKhsYOq7qwehCjmRIRdWKVmREMTASyVDLSQhcxNnmWvB+qy6FLirGMNSWpMui1kscHS+Tw9j4y+vq2ttP2aKPr6fL+fbiSqCvRIBF6EXVilZjc3e8h5Uaibs7naB6kXMFdnUfAaiyrx3amlSpy1r0wm1US3yWtuanHu1PvCqct0utqzSrEoejJrNAci7MIqNbu5WXvJjpftFjbU+VnBGzjtQpj0MnV1hYwjurli2vteYveEyO/b51oUXU6Cy4wIu7BK5AGP9Vazvh9cn7GvQr4sh0MvROW1xaJnuq5nHwrXouhJZsYciLALqwS6pJzeatYebcxnPJutdr+B4/7vJsRiOq2W8zynWNmfuaJfqtYxp099bS08VtF0EYvdW6IbNFFE2DvEYwVzn/1sY1Bt+MlTi240huD2OHXK3fBxyvp6PkE2I1ya6knUTe0sPnYR9pNK673VvolnKJ+Lr+iWLecydilla6u65W6GIvap+GL661oUEhUjwn4S6aS3mttnbLpROCsKcS4yFDOvxWGIPm8zL3uX7hSzxMJMBbawr+VeHFsYJq7F5jc2ys8bY2cHuHwZODgoV76vClG5/8c/Dty4UcrEAw/w9l1fBx55xL/6/NWr7v2UKusPAK9/ffW6d8X2dvnvzg7w2te2d96NDWA69X9vPhOXLx/dYyEJEXYBQPn+XLxY6qPWyYsXM71Xu7t+4dQ4Wpb9U1McxI49nwOf+lT5//e85+j4PkG22d8HPvYx4MqVUqyvXAHOnTus463Xbzt3UwBw6lR5s/7iL3jn6oLZLN5if/e7vGMRVa+H+VDdvevehlsPIQ7HrM9dxBUzEHJ0i1MGrozz3ZzN1ePrhdpHwM2ho0VSltxLdM+cnxXqFno0FpBSuP7pNnztnPMNMEqlbdDimqcPAPgigD8B8DUAT8T2EWEfAHUjCWKDj5HYeb3rC/AcQy8D5zuHK5IipSx90ERKnUWhrmOmDroWak7xLdwRS5HQ5DiBzmtv/l4paXnF735Im8L+QwDetvz/awF8C8CDoX1E2AdAHauKE/FCFGw79Ht/Fg6L2RSBUDhPUaRlJLQbhuVtcNahj2Vz098TimU5bLpurnkJ+rebzfwpf0cUqpiD1oR95YDA7wD46dA2Iux5qWrQBPerE//I6NrfnM2DGXXNQ5xFoV5AmVPm2sSqaMhir+OOWQp7USj1InjHeRVr3Vv15oLWmtB9SFkHNcd9dR2vrfVPR0Anwg7gNICrAH4wtJ0Iez6qGjTR/eq8UJFu/d3phnp8vfBuoo1t1nW5VtthTsDZp0BIpBEOyBHrW9hQ15kNQOPFTMlbFNHtb87mvLGE1EagTtHP2YjSAeSgdWEHsAXgWQA/5/n+HIA9AHvb29vN34ETQlX99e13aPDV6QKHXvzZTJ2f+UXdrHu0J+KqI2fK/NINFByc1ZUIWKcHgNoH1AuYq6ex6N5aN4uGYV3vgxRwvGd0czYvc9a4VpVqI24/9/qnI6FVYQewDuAZAP+Ys71Y7PmoatCE3s1D/a7j4/H52Dc21KPwC3uS+7Sq5TifH+76NBar4m5GkwRE/UXM1FkU/fTBr62xE6W9gPnKx+dnAb98LlfMdOp/EM3W3WdgnMBB1TYHTwnAvwXwYe4+Yxb2tp+13BY7d/8ogan2LiEB3O5h83DzuVKPolDXJnN1EAqDDJWlKJh6YlqqL2CelHf9ZWrJBdNQ2oJ9kHoai5V7Eex91KnL5mY4yZsp3PaPb75UJ3RQtU1h/wkACsBXATy3LI+E9hmrsHfxrOX0sZsliwvTY43prj+3zrqutS1jIwok2CA25EfWrhvzs7soB10PBfPBB91jBg1mnHwFU3UdM7W//G0qH4sj+PqH9l0Pp3XnrGU7UjqLiuGUsQp7V+7AOh6TRt8Pzw2xLfZYnaMx7bFinCAWSn1p0Vxysn3Qsd7BdczUK5iuVsJM0Vs1XLOrwsk4GWqkfBYFN2ncyAdVRdg7YIgD+I31MjxW2S1sHC6Bx703+r5Wsibnc3VpUbpv7CX4THE/1P4GIz7uYnIo6NEJT/O5O+Kn7yUl+Zrvul1wfxex2KGUCHtWUi32voz9ZK+Ho7U4ANT15WBj6ntYx2Iv3R/HxdFuXI7VYWhCOrZiTppSib+L+NgPS3SDJspYhT01LUob/visos09mKeFu0Jz9vWap3r/ZqEuY17JB+yzik130LFeQ67VhRh1GGRpI/e8+WDExjv0OrQSFXOsRDdoooxV2JXia5/PzZizJ5m18eAcLPISHoAOv/alNLFP5Row9Qml/XmoETAHcA/v+WLRvGj1sXB7KYtFe2vN6gcj5Fc/ARa6jQh7jwlNBszpj886mBuauu8bjXSc2BbtYxNili/p+dnR53fBtxD1ZCHtS7cjUMyiLfbH14vy3BVcMKOwxLe2+EnAdD6XtuoW6h1wLfS++DszIcLeYxhJD7NQdTDXNLz1WFitMDjg0LrSx/Um91osKoc0XsfsmLD74ssPAPU0Fur8rFCvnKp2rgNkFHYipd7whjzHqlIWi/ZXUarTQHCtnxHGuouw95jQM227FusYGhyL3T6PK705UCPUEDg2IKav3Xs85gtvNzR3sL4SOvgKpn6rfT4vLfU2xWzMxfR1+/yMdSNmjN8u2wswMETYe0zIq6FUPkMjdhxfqhVX3WpNDtJrkRKpq2tluGGdHsAtbKinsWBb567P90H1eyGR0rSrZuX4TbpJOIuXhGaFTqe8GHdOPbgvwhDjjyOIsPeYmODmNDRCln9qyLY99b7qVPoqmRB1DLgdh65LqkhfxyzJf9/HcgCoO5stpTNYLHiThHx5XHLMnE3tusbGhQbodxdh7zkhwW3L0OAaeLaga2GtY8WHJujYn9tx566S4ipyuW1Oeon2LlyCnTJtuW5vosrDz+05DMjvLsI+YNpyDdrncQm4S7xNobUXwbizxbPM9AxM13f2IKg+V8gb8P7NQr0caWQOgFo9jbGWVzBV3wMjdYH9AIa2tYWSY7FvbPjnEVR9+Lk9h4H43UXYB0xOH3uoxxmLFw+5TFwZGon4iz9rwQ41GvZ7F0rqd2lRqBs0C0ar6GiYKr71mEWbNUqmhaLrez019bB+oGJCbT+wse31AHvTkSwD97uLsA8En/jWjYrxDYzqtRLM7QC/KyM0+Gh/rHvmR1a8e/87WD8Ubx1H7vKf++Lc7XuTkrjrAFC3cU9lMQx918u87J6ifwM7KVmwN5PqTjEjZWLbmsnPzIcptw984JEyIuwDoK5xUmVgVC87ZzKf+wcfOVPyfcXXWFzHbOWddbmFgotYmzchcZp7Hcs61BsgUuoTmz1bSSlQrmPmntXrEvAcPvLUbfTvvVi4V3KqQsoM6h4OroqwD4A6xkMoQCGmc/bxi0KpK+SujOvl5wxmAv7GYn+ZWsCug7mZr1Ew88082oGF7BXtzU1VFKpcbLvF+ug6VWlM2PvkCqOschyfz72uuPuEu+eTmkTYBwDX3ed6Dutkl3W5Ey8tCvUyuQXcFxUTKz5x1ta+/a6YqcdDjULs+E2Ww0UxrHLn1Kba2FC1ewOp+x8A6nvYCqZPCO2b9NDkuIc5FwxpwqLuuatGhH0AcGeG2gZE3Xke3me08Octr1I4g6M6VYlSx9/5WKNwFkVYmBqYrHMLG14B1QOzVYU9FCXE3T/39UZLaqbH2ayZRUxO0OCqCPsA4PT6cq/7EHsHOHNQQpppW/f2DFFXYzGdruahCjUKZ1GoOwi0bubyarZ/tsKKRGYEyffgT+tbR9SrRuvYx8kumjlLk6tB5bKoxWI3DgJ8HMB1AM9zthdhPyI2TpPT8ExNiAe4x7PMldvM71LCF+1yfrbaW/C5gIIuGNfosH1xNbo8VVwevqLj6lnXxaxbcuNyzz35crh0WXJZ1OJjPybsPwngbSLs+cllsdeZ32G6SOwFbjgpdmMRNKkNQtQyZV5Q17HnerxAN5B1XDn6PvsGwV8FDSZap1LJvZCBRMUcivtpEfb8cFwjsVLH4PAZMIsFfzLSPkidOnX8Y9MaT20QgpatzqTGvJjOE4HNZurSokxnXMdi1z5+V6qGW9hQj68X6vvTRFdInyz5+Tzs0++R+DaJCPuICLlGfCXXimGheHiuEN2clQts6Mi1qI98WcwIGLMhULPAQtA+YQ/Eu3duya6vh1dfYZTvYdMZk34AqBs0U19/qMLqUOvreaNYXMVuPNbXV3PDa8skdJweW9k56Z2wAzgHYA/A3vb2duM3oEuaeMZSLfdcLsdQQ8Kydq3uQlEodYMZ/XGUM2bVsvaKsevCi0LdnVZfUCPl88olZJFOJpElB9Ft/pvFonoD4Mq06HuBQtkae+wXz0nvhN0sY7XYbX90zmfM90ynJNgz68lteEI+fp/FfhcTdYDAwRkvfOUMjI4Lr7OgRqMTf8xCfh/4AbAaNmSUMo69o4iYzc16/sIUC8TnFxx4Yq8URNhbJvZs133GQpZzirHCnVHNMcBCg556kNXMGaWXzAwJWEqud5cv2XXhbYleneyR+2sT737XJvPyujzL172C9eFmrEx9MVxWSZ9jzzN339uOivktAH8O4C6A7wD4xdD2YxT2WPRKnWcslA7FnIlaxwLX71dAPw63M/8OzUr1RRWG0vXqPzlibKf3PT9bvfCiqB9GaDcgsfw5Z1FU6m28gunK2MMtbKiPYBGfDLS5Wclq7nR8IZe7pK+x5w2ETsoEpZaJDWqmPGO2pesTSN9iNaHnJrbeaqiB2txMn2zoKi7hewXTY41CTIzNyUo6EuQAUHe2Zur8rDiWfrtu1sWX8ZqouJdT+zd5E6gCxZWL/gYxrfGKvu6yp8QQ+ZyTKlypRqvS19jzBhocEfaWCQliasZGjuGlk9yl+vRD730Ts719JZZ/xiXG5TqlODaBySWgdiOhj1fFOr2D9aT96sbG2+mQQ66rleJLmMUod3EqPPlKx7jmaNl1cQlcVddFH6NiGnARibC3jE+Q7Qk9MVImJIUMKN87E3Kz9K3ExD9k1bti4DkumQOj3MWEt7JQleL58cx6Hxq1Xf8QwFFSn9xWe+wl6oPlXRWx2MdBDqMhZ4ZUm9x5Z7ouIQvTtRBIzCWzD6i/tCZLNeKDXna3fNk09btfFKp2fHvWEopAcZVU/2RffeVVER97/+iqZ8cRX474u8KDG0h4WLm85jX19o9NvzctX9cqQfsgdYNm6sby/6nRLLUEf9nqXloU6rLVI3lsrVxR6vCHazJxVpXiy9CoJxqZKx8tFn7/nsvH3ufolqoMOSomtfRd2LvsEbrOPZ0ehQpyDCbf5L2mJxGmFFd9uHnfz6IIRs2YPnbXYhy+PDShlLzm3/sg9QweUq9WDaNcWqC2ayx1kJc96Jmz6CRrodF982UJjchzU5naeaz75ktvERH2GnTdI/Q9u5yBVR0v7vpua2vVKOqLFZ+SCCzkKz8A1KOGO+PGlntb3XDoma3lpCp/Be2B21OnKg7IGmJmP2cpYZl6jdXU/WqXqu6TqosP2I3EmHzwFRBhr0Ffe4QhN43Zs+2LWKeU2MIaumxuhmPczfVUQzdjH0gOgbTrMpm4XUJesbfW67SrljKR6nvYPPyzblZIdnGlROa+LHWWC4u9AEP1wVdAhL0GfX1+YjHomi4GSes0JiHL1zUIGrJQtRvmUC88N8OXUTJUXHXR9deWf0xgv4/pYdz9XUyO9QK4lverp6bqo2tHC5hUuZbKxYYz4y30QKa8VH21uFpEhL0Gfe3xcRucLgZKt7aqxcHH/MqusEXOPof3xPFjVo01t+uytnZc1OukLriFDfU0FqvXpePHTf/a1pZ6dVJzfcQqxRdD63tZYr7D1JeqCYtrYD57Efaa5Py9cx0rpcFp+503x9T031qEq8aihxbbiFn5x+YPFIW6sxlI9esorjw0nElUdcpxvz+pO5uzWpOOspZYAiL9w5tRMaHR+iovQm6Lq68WXIDRCvvAGthGnkVOltNUdww3IiVU7IFbzoCoz8o1Bwd9JeaX1/d5sUgbYNxfrjZkukoeTRzErVL2QUluHU6pdAwdqpjyoqVkeKybOCmXAPTV5xpglMI+wAa28WfHd09CIcR2qbNWaRXhLQUzHDseW06PW28dBh5qQPZxJOL2drewoXbIt0RfjZvjqUvuQdDWhD3FkuiLcA7QZz9KYR9gAxt9duoaIL57os/LSe3BjUhJLRyfsy+jIbdR4fY0Qteo77svb7t7slNcNDtfmQnwTyiqeizfA8od1OFYYm11ywcoKKMU9gE2sMFnh5tfxufCzDVI6hNgXxSIq7gEluuqcGU0zK1vPuv+E5sLdXM2P3S/hO4Dx6du9gCyX0SVslgoVZSzWbM0ND5h9j3oego0sPrwOri0KFZSLTTWLR+gC2CUwj7ABlYVxerEPL3EJScjZMh1mWs2aV2L3SeazigPR0lpQOoUu/Hh1k/fB05DtQ8ooL3FPWLl2mSuiEpNzTYmkCM6xiGgRaHUFfLUsamXfGCDdqMU9r41sJxnwpVRcTrlWdvzeTsx6XV97KGGwRRTnxVb1+VjX4vP+re/4+SG0QOaAE+sUxqBNoq9IHiWKB5fF9lM62tOxmJaZPN54B73uVveIqMUdqX608ByG5nQMx0TbaJ88ehbW2F/e52oGK4rp6lBWs7xXd9xXBPaAgf4i38ASj2+vrqA9ivLyUkht0+o5Ii9fxoL9WrF8x+WVIud6UMlCtzjPnfLW2S0wt4XuG6h0DMdixDLZbFPp/5VmHKUq2vuSvomFzXlTw/1HKpa0OZyfb4l7w6W2zlT7i6tEPtag/VZxo3aIZepYmw3mlks9lQfe+ghdljsrjq+TP32e7dJ22uePgzgmwC+DeBXYtuPQdi5A7k+H/hsVn5fFOFVkFLCg33vTlU//GRy1DMKzZMJWcq5AjJcvQ27kfCFH5YJvPzhjqET26sx+dw3OoTzxtbcKUK2tvnu2fteU6hLi6MG4eZsfrjU37XJPFhX85pSJ4P5yp0tR/7n1Bciwce+sXH8d71C8/J+CEoppVoTdgATAP8LwF8FMAXwFQAPhvYZg7BzLfaYsGtCLqbFopoYmgEJVYqZLjvmEjJfxqtr5YQeHQ5d9fyx87mWznNtHLLYzYicV7Hm3V//yfGz350eiZZrNu7qPTuyzK9jttor0BneCkfEiKP4BqNTB3RvYcO5QHilF4LpQ+2Lq7WvtCnsPwbgGePvJwE8GdpnDMLO9bHXDdGsarHncL+YjVRKA2Heh6YGf31C7Zpg5POx68gdLeycRGRsq3c+d/52+nkwUy5wI4ceRaH+zXQRFWjfYDS37qbFzx6zbDOy4QSrf5vC/vcBfMz4+z0APhLaZwzCrhTv+aobollFGOu4X2wR0iwWaQO5+vpyCbld/AO2YEfFpIY7pgixQhlq6KqH6VZKcY9wxgpCqRg4A8i2Xz5pzLINwe1baFzL9E7YAZwDsAdgb3t7u4Vb0A/qPoepUTGTSb6JS2Yvukqvoaksk2dRZAmd5IiqL/lXNGUuuXsO9mYp7pHQWIEuerDXN0htfn5ja36YOuBg6c8269hLvRziZJaMiCumR3Dj3VMDce5IAAAVc0lEQVQT5PkKd+KSrlNodbOi4KUlcJU1t8u6tqj7LObU0Mlw/hhe1E6qr9/8fYG8FvsBoJ7Ggh1WamvhIDwcQ5x+npE2hf0UgD8F8CPG4OlfD+1z0oQ9RsgidvnKOcIdSxFiRrz4MkTWjchpooQSi6WGTqbOuPUlV0uJzrGPwXXt6OgcTi567nWFtLBXIm9WxmdliMV+XJc5G0UPAjwC4FvL6JgLse1F2I8T86NvbVUTXV8opUuw7Be3jqXeZMmR18YUZO6EKVPciiJ8XJ+wXpvMncfgrJ16B+vHJj/5tg9liLTvkU8Le+XG5jzovfQZNYNMUBoQHD+0z33j234yOdrGFHffuWaz464ZOw2CXfRxYjHuuUuqla2vJyTunAlTttUaaox9DYYdj20eg+OSMTNRVhm8MO/RdKrU+zePrv3m7OgCO3djcyx0u8t5QhBhHxCcyBffSxXaR+ciz130AK1S7iRnVcWTU1LTEug6xo55mVE/c8HwUCij75ptw9I8Bi9zJPF+eEe5O904nOg0myn12Nrq+XT8fadubK7/74T41G1E2AcE51n2PcdtJAmzi5lWOHb+JvLDpDQUsQVHUuunJ1IWxfGeCpFSDz4Yr7tvbMO8Lp8r5dpkfvTDp9wwy6qdz8M5Wez66PvMnqxUB+4DfUJ86jajFPZeDehkJpbGN8Uf2kaJ5XfSpalFPHKVKvXjuKpS7qH9O55FsbL4yB2sq0dhPPDccCnHg0MUzqJYFKUf32fRNwrHxXSCfOo2oxP2Xg3oNEiV64w1Ck0VTpKynIOdTZSu62fqrv4dXcnGXsFU7VBxaNRcWjjyQTMFMGaxK+VfTapxS9n3QJ1Qn7rN6IS98wGdmtSJZfctP2kfs+ps06rx5totETKy+myxhyY63ZzNG5lc5buH5u94Yyt+zzY21LFkYcEHxfGcRS3yrhztJ8WCq8johH3I8xI4z6pvG5ePWH9uG2ynTsUHMu0gA9+xOEU3qqF0A03nYK8j6t7ByuWP89SD7gHQHOkadNncLO+d6c8OhSua212bzCsLXlEodX7mjopRStW3pOr4Tcfsc63J6IR9yBa7r+6c7Iu+aC+flW3GvC/Teh87zubm0b7mIjehLIS+Yg6ihtxBTeZgr1q8rohlyM+lhbtB+s2HCm/Gyth12g20boS5k5SuY+beLrTIdFXqWM6p+4qQsxmdsA+5h9ZGl94ssZ5ATBdi4YGh36CLwVxOw5EyO5RIqcuBSUauBozbM3Gli+DmrAku5dfEy1BVcGPWiu1PHOqL3QGjE3alhtuw1xnYrDL7s2ra3JQwRns/k9hgbs4ZrRxBrZLPJXVQNWUswY4oCib2Wi628dG1RXwFpZTua5MvU0pky5C74h0wSmHvmqrvQlUr1udj5xT9XqT2Fqqu3ORKSeBbGSrFp7+2Fp5oxRHU1NztqUIdEmdfQ8BKBGak17TXT3UWY8DJNxBfFJ4H0px9VZeUWPQhD551gAh7AtyIlTo9xtQIFnt2Z6rVr6MtqljHZrpebp1Nq93XKNg++dR6VRXUqrnbUwZ9fa4bTvSPb53PwxQEzB//5uzITeTTy40NfyjjPijPMnQps0fFYk9ChJ0JV7BzP3+hZ39tzd1gpIj02lr1FZR8xlJMjGNuHPte5XDJcJa884U0Xo6ILnfQ9ywKdYNm0UUrUs+ln8MD5nJ8j6/zzhVy/VyheR6vDMeq0FaU+NjZiLAz4YpQEz3GovC7GVy94roiyC2hxipktcfcP/a9ylFXl7X7CqYrMzdt0X2Z0kMuXZFIZ1Golx2rEl3HLEv0z2ym1BXyPKTGpJ3zM/65QoO1+6B8xnLIerGn3Q5x8KwDRNiZcEWoqR5jKHkdd9ucRfvAfe9ZyGrX9yw2edAMw8xRbGvXGz2yrMDNWfWQS3tsgOOL1+MEoWSFsetzuWrMHyZlLOX8zD+r7AXM87q3g85+IRURdiZcwW6qxxh6AU0RJGomW6OZrldPXrSvczo9qsN87q+H6ZvvaoGOtbV4HpQ6dbMbpJCP35fn3vUcxRo6u/F6FMUxS/fahNdYTafLOi1WF8XWriNxb/cXEXYmKYLdRI8x1YJzGVp14uRtlw9nnM63qpNt2ee0ylMKJ3NhleJq0DiLanCeo1TBPz9b3YHj0zcHuS8tCnWFwqmFhX4hwp5Aly4+3yzG2Mtp1jcmRtrydzUA6+tHwuKbeOMaQLTr4LJMc7mOUsU4tNBFnUbQFaLpnDVaUR1TBN8X2XJtEs5xY7tZxL09LETYB8RikSaC9svJCUMMiaNvfdRQyJ9ZB1scquae8ZXYNfrE3W6QJpNmEqU9ikDOlQjcUNuVvC6Rh0OiCMdJK8IO4OcBfA3AAYAz3P1E2P1wrFP75YyJlTnLMaWEBgZ1nH3T/nTdUOQ8Zmo2y5R7lzLPh+0G9E0oCjwcEkU4TtoS9r8G4E0A/qsIex5iQul6OWPCM59XS2sQm/yTO9Oh61pDmSPrFHMwOPc16MlhMdhWtW9D+8ZYD4fZEzQTvgnDpVVXjAh7XszuuRkV4+uqx0RbW5Gu79bX/cLWVS5106XTVIinKZ5NNBy+2bvm78cNtQ1OUPIcXCz2cSLCnpk+DzIVRXiWqc+HDpTi6esldJFLvc2QySoWu7mQT2g/bbWHxJVjsRdFYIJSwGEuPvZxkk3YAXwBwPOO8m5jm6iwAzgHYA/A3vb2dlv3IQt9t36Kwj9YGXOX6OvwWcdN5VJ/6KFwPpk6YYlNNwbmfQ+5umPiynmu5vNwlI8Pya01TsRiz0jfrZ/QTE/OwGPIom+qaP+5LwNk1wLuK/Zv7hoDiC30HYoo8o2fuBrY0PMXemb73PsUwoiwZ6Tv1k/IR9yWSFZZNzVk1VbxrTe9oEnqxLUcBkGo5xJ6/ny9Ad9SiyLuw6CtqJifBfAdAN8H8H8APMPZb2jC3obFXseKatNtMZu5/fmTyao7SKciaKOx4OSfef9moa6upbmVdB2qWLY5XHgxd09sX/uZ6nvvUwgjE5Qy0rSPvW5agzpx3qkhi6FUAb7ZqFUanvk8TdjX1srFvH3f1xkItnPlpApzXbdHyN2TSt97n0IYEfbMNOmXrJuILDU5mBnZkTLJaHOzrEeKOIRSE/tKKDyzauGGbqas6pYL7uzTHM+fWOzDRoR9QITExHyZc0yk4fQEfJZyLD2BqyGquthH1f18hbPSUsr9zSWEbUdc9T3CSwgjwt4wOS147sTCqmVtLa2eofMqxXcNNOX7r5K+OGSxV3Eb5XJddGFBS1TMcBFhb5DcVk9KKhCfcIe+N1O1cgg1NK6oCld+lKJoRtR9E6rW18OJx3w+9vOzYiXdMMctVUV4XYIqPm8hBRH2BkmxsrjWkb0dV+h0CFtI1EJrmLrqFvJv+8IQ7dmSMXGs0huJxX/HJjb5Jlu5csmbKR1iuec5+IwBn/tHfN6CCxH2BuFaWSFLPNYF9gmUL/IklP/c1+D4eh1VXCj62kP10GU6Pb78ni+E0tWocAS1SjqCkJCGGmduwx36PcXnLXARYW8QrsUeE8jQC1zF3eOztF1Z/ULXELKmfaK9tcWzwnXKANf1mkIfSpHAFfc6Oe45pPxG9r05a+VwPz8rxOctRBFhbxDuC80RuqqWoosUF1Go15HqYw/Fj3Ov1XXtKT0Q1z1zNRBdTfYxt8256lIdZBB1eIiwNwznpeC4NHIOkqUMxMVyiYQGSO1rbyr2O3Vg0Teoak4uqjKl3teQx65ZN0z2eUNrsraFhD0OExH2HsDx9eZ8l1MHdUMvdoo1xxG4KoKRGgqYEl+foyeU4urZ2CgzWk4m/ph63WK1YUnLRKVhIsLeE8wue65p4aFzpVhhqRE7thWqtw8JHMca9g0G+zI/+o7XVOhgqEeSMpFKHydksbdlSde+V+LH6QQR9hZIfbbbeBdynyPU6zBzufvEzDdYqo/tShzmcpcAZbx+aHm3pqzQUERLlUW7Qz72tizpWucRP05niLA3zEl5tmPjBFoIQsvY+e6LL4Y7NOEqdyQRh9QYdK6466gYswVua8JSrXslfpzOEGFvmCE923Ws+NjAqC04KfelqijmjCTikjJr1HefONfS5nNV+V7JdNnOEGFvmKE823WtWK7Frkm5L1WFPTaTFnCPBXDvF1fsYoOqvqiY0O8wiJ7gkKyakSHC3jBdPNtVLKy69eT42Kuez+fK2NwMRxNxo3w4whiLe8/p+kkdrO7tuOQgWp9xIsLeMG0/21XPV7dnYUenxFIipNSzKFajStbXy89To2JSexa+uqY2gr0X4aY4sRfeLSLsLdDms13V8q5jsXNF2r4PZh6Y2H2J3UPuPU4dCwjdm6qNoCA0TVtrnn4IwDcAfBXApwG8jrPfWIS9Tapa3nV6FpxGoS+9cq7FbjYUHFHnNoKC0AZcYV9DPT4P4M1KqbcA+BaAJ2seT/CwvZ32uWZnB7h4EZjPAaLy34sXy89jXL0a//zCBeD27ePf375dft4mTz0FbGy4v9vYKL/f3QXOnQOuXCklm4PeVxCGRC1hV0p9Tin16vLPPwRwf/0qCS5cwsUVnZ0d4PJl4OCg/Jcj6gCvMeGIfxuYDRgATCblv2ZD5mqEbNbXgdksvRHsgt1d4PRpYG2t/Hd3N+/2woDhmPWcAuA/AXiMs624YqrR9ngVx80ypMi3kPtFR8WYCcP6PB5YJSKnDy4zoR7I5WMH8AUAzzvKu41tLqD0sVPgOOcA7AHY297ebu1GCPXgDG4ORTBSM1r29TqUai5BmtBvsgl79ADA4wD+AMAGdx+x2MeDHZYYyg3TNSHxThW+rqP9UgfThzKhTgjDFfZaPnYiehjABwC8SykV8V4KY0MPRr700tFnd+7w9svl6005VmggOWWswB6EvXKl/LtNn3XqYHrVwXdhoHDU31cAfBvANQDPLctHOfuJxT4OQiGGOSYwxcg58zPFYu+DW0N87CcTyAQloWliseB1Uw7ESDlWTNhShK8vbo0+po0WmoUr7FRu2y5nzpxRe3t7rZ9XyMvp06UbIsR8XoZYatbW3DHkRGU4Zgopx/LV1azf7m4ZEnn1aumieOopd6gj51iC0ARE9KxS6kxsu7oTlIQTzFNPAdNpeBvbR53T15tyLI4PnRvv/8gjZeNhIhOZhD4hwi7UItbhs0W2zkQrm5Rj5WpQdneBT37y+HUTAe99b38nMgknDxH2gdGn2YMXLgB37/q/d4lsnRQHNinHytWguGavKgV89rNpxxGEJhEf+4DQYXamsGxsdDft3efjBkqR9fmou4LrQw+Rc4xAEFLh+thF2AdE3wbt+lafNjiJ1yz0Bxk8HSF9SbilyekvHwon8ZqF4SHCPiD6Nnswp798KNS95j6NkQjjRVwxA6JvPnYhDfn9hLqIK2aEDNFCFgv1iL4sSiKMH7HYhcYQC/U4ElEj1EUsdqFzQhZqU5Z8n3sIfRsjEcaLCLvQGL5oHZ3mNnfa2z6k0w0hETVCW4iwC43hs0Qnk2Z8zX33YQ9xjEQYJuJjFxrD52P3LShd19csPmxh7IiPXegcn4U6n7u3r+trFh+2IJSIsAuN4kqF25SvWXzYglAiwi4c0lZESVO+ZvFhC0JJLR87Ef0agHcDOABwHcDjSqn/HdtPfOz9Q2LOBaH/tOVj/5BS6i1KqbcC+AyAD9Y8ntARfY8oEQSBTy1hV0r9P+PPTQDth9gIWehb5khBEKpT28dORE8R0TUAOwhY7ER0joj2iGjvxRdfrHtaITMSUSII4yEq7ET0BSJ63lHeDQBKqQtKqQcA7AL4Zd9xlFIXlVJnlFJn7rvvvnxXIGRBIkoEYTycim2glPrbzGPtAvgsgH9Wq0ZCJ+gB0rpLxwmC0D1RYQ9BRG9USv3P5Z/vBvCN+lUSumJnR4RcEMZALWEH8C+I6E0owx2vAPiH9askCIIg1KGWsCul/l6uigiCIAh5kJmngiAII0OEXRAEYWSIsAuCIIyMTvKxE9GLKAdbm+JeADcaPH4dpG7p9LVegNStKlK3dO4FsKmUik4E6kTYm4aI9jiJcrpA6pZOX+sFSN2qInVLJ6Ve4ooRBEEYGSLsgiAII2Oswn6x6woEkLql09d6AVK3qkjd0mHXa5Q+dkEQhJPMWC12QRCEE8tohZ2Ifo2IvkpEzxHR54joDV3XSUNEHyKibyzr92kiel3XdQIAIvp5IvoaER0QUS+iAojoYSL6JhF9m4h+pev6aIjo40R0nYie77ouNkT0ABF9kYj+ZPl7PtF1nQCAiO4hoi8R0VeW9frVrutkQ0QTIvoyEX2m67qYENFlIvofSz2Lris6WmFHv5ft+zyANyul3gLgWwCe7Lg+mucB/ByA3++6IkD5kgH41wD+LoAHAZwloge7rdUhnwDwcNeV8PAqgH+ilHoQwDsA/FJP7tv3AfyUUupvAHgrgIeJ6B0d18nmCQBf77oSHv6WUuqtbax52lv6vGyfUupzSqlXl3/+IYD7u6yPRin1daXUN7uuh8GPAvi2UupPlVJ/CeDfoUwP3TlKqd8H8N2u6+FCKfXnSqn/vvz/TZRC9cPd1gpQJbeWf64vS2/eSyK6H8DPAPhY13Wpy2iFHeAv29cx/wDAf+66Ej3lhwFcM/7+DnogUEOCiE4D+JsA/qjbmpQsXR3PAbgO4PNKqV7Ua8mHAXwAZRryvqEAfI6IniWic7GNBy3suZbt66Juy20uoOw27/apXsI4IKItAP8BwD+yerCdoZTaX7pH7wfwo0T05q7rBABE9E4A15VSz3ZdFw8/oZR6G0q35C8R0U+GNq670Ean9HnZvljdiOhxAO8E8JBqMeY04Z71gT8D8IDx9/3Lz4QIRLSOUtR3lVK/3XV9bJRS/5eIvohynKIPA9A/DuBdRPQIgHsA/CARFUqpxzquFwBAKfVny3+vE9GnUbopvWNhg7bYQxDRG40/e7VsHxE9jLLL9y6l1O2u69Nj/hjAG4noR4hoCuAXAPzHjuvUe4iIAPwmgK8rpf5V1/XRENF9OgKMiH4AwE+jJ++lUupJpdT9SqnTKJ+z3+uLqBPRJhG9Vv8fwN9BpDEcrbCjXLbveSL6Ksob0YuQryUfAfBaAJ9fhi99tOsKAQAR/SwRfQfAjwH4XSJ6psv6LAeYfxnAMygHAP+9UuprXdZJQ0S/BeAPALyJiL5DRL/YdZ0MfhzAewD81PL5em5piXbNDwH44vKd/GOUPvZehRX2lL8C4L8R0VcAfAnA7yql/ktoB5l5KgiCMDLGbLELgiCcSETYBUEQRoYIuyAIwsgQYRcEQRgZIuyCIAgjQ4RdEARhZIiwC4IgjAwRdkEQhJHx/wGQHWhVUNBqHwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate random data clusters\n",
    "train_perc = 0.7\n",
    "val_perc = 0.2\n",
    "test_perc = 0.1\n",
    "\n",
    "cluster_offset = 1.5\n",
    "N = 1000\n",
    "num_train = int(N * train_perc)\n",
    "num_val = int(N * val_perc)\n",
    "num_test = int(N * test_perc)\n",
    "\n",
    "x1 = torch.randn(N, 2)\n",
    "x2 = torch.randn(N, 2) + cluster_offset\n",
    "xtrain = torch.cat([x1[:num_train], x2[:num_train]], dim=0)\n",
    "xval = torch.cat([x1[num_train:num_train+num_val], x2[num_train:num_train+num_val]], dim=0)\n",
    "xtest = torch.cat([x1[-num_test:], x2[-num_test:]], dim=0)\n",
    "\n",
    "y1 = torch.zeros(N, 1)\n",
    "y2 = torch.ones(N, 1)\n",
    "ytrain = torch.cat([y1[:num_train], y2[:num_train]], dim=0)\n",
    "yval = torch.cat([y1[num_train:num_train+num_val], y2[num_train:num_train+num_val]], dim=0)\n",
    "ytest = torch.cat([y1[-num_test:], y2[-num_test:]], dim=0)\n",
    "\n",
    "# Visualize the training data\n",
    "plt.scatter(x1[:int(N * train_perc), 0], x1[:int(N * train_perc), 1], color='b')\n",
    "plt.scatter(x2[:int(N * train_perc), 0], x2[:int(N * train_perc), 1], color='r')\n",
    "\n",
    "\n",
    "# Randomly permute train data\n",
    "permutation = torch.randperm(xtrain.shape[0])\n",
    "xtrain = xtrain[permutation]\n",
    "ytrain = ytrain[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a07a076a2105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Set network to training mode and feedforward through it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "# Number of training steps\n",
    "max_steps = 10000\n",
    "lambda_ = 0.0001\n",
    "\n",
    "# For L1 or L2 regularization\n",
    "# Uncomment either l1 or l2 to be applied to the network training\n",
    "reg_type = None\n",
    "# reg_type = 'l1'\n",
    "# reg_type = 'l2'\n",
    "\n",
    "# For dropout regularization\n",
    "p = 0.0 # percentage of units to drop out [0, 1]\n",
    "\n",
    "# For early stopping\n",
    "early_stopping = False\n",
    "prev_val_loss = None\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\" COMPLETE CODE BELOW \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# Now, let's build a four layer network for binary classification\n",
    "# Hint: Last layer's output activation is Sigmoid\n",
    "# net = ...\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\" COMPLETE CODE BELOW \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# Define optimizer\n",
    "# optimizer = ...\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\" COMPLETE CODE BELOW \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# Define the binary cross entropy loss function\n",
    "#loss_fn = ...\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Train\n",
    "for t in range(max_steps):\n",
    "    # Set network to training mode and feedforward through it\n",
    "    net.train()\n",
    "    yhat = net.forward(xtrain)\n",
    "    \n",
    "    # Choose type of weight regularizer\n",
    "    reg_loss = 0.0\n",
    "    if reg_type is not None:\n",
    "        if reg_type == 'l1':\n",
    "            regularizer = lambda x: torch.abs(x)\n",
    "        elif reg_type == 'l2':\n",
    "            regularizer = lambda x: x ** 2\n",
    "        else:\n",
    "            raise Exception('Unknown regularization: %d' %reg_type)\n",
    "\n",
    "        for param in net.parameters():\n",
    "            reg_loss += regularizer(param).sum()\n",
    "        \n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\" COMPLETE CODE BELOW \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    # Loss computation\n",
    "    # loss = ...\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    # Compute gradients using backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Take gradient step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Check early stopping criteria\n",
    "    if early_stopping and t % 100 == 0:\n",
    "        net.eval()\n",
    "        yhat_val = net.forward(xval)\n",
    "        curr_val_loss = loss_fn(yhat_val, yval)\n",
    "        if prev_val_loss is not None and prev_val_loss > curr_val_loss:\n",
    "            print('Early stopping activated!!')\n",
    "            print('Previous validataion loss %f.' %prev_val_loss)\n",
    "            print('Current validataion loss %f.' %curr_val_loss)\n",
    "            break\n",
    "        prev_val_loss = curr_val_loss\n",
    "\n",
    "\n",
    "# Eval mode to turn off dropout\n",
    "net.eval()\n",
    "yhat = net.forward(xtest)\n",
    "colors = []\n",
    "for val in yhat:\n",
    "    if val > 0.5:\n",
    "        colors.append('b')\n",
    "    else:\n",
    "        colors.append('r')\n",
    "    \n",
    "# Visualize the testing data and the predicted classes\n",
    "plt.scatter(xtest[:, 0], xtest[:, 1], color=colors)\n",
    "\n",
    "\n",
    "yhat = net.forward(xtrain).round()\n",
    "print('Train accuracy: %f' %(yhat == ytrain).float().mean())\n",
    "\n",
    "\n",
    "yhat = net.forward(xval).round()\n",
    "print('Validation accuracy: %f' %(yhat == yval).float().mean())\n",
    "\n",
    "\n",
    "yhat = net.forward(xtest).round()\n",
    "print('Test accuracy: %f' %(yhat == ytest).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check what the true test data looks like!\n",
    "colors = []\n",
    "for val in ytest:\n",
    "    if val > 0.5:\n",
    "        colors.append('b')\n",
    "    else:\n",
    "        colors.append('r')\n",
    "    \n",
    "# Visualize the training data\n",
    "plt.scatter(xtest[:, 0], xtest[:, 1], color=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONGRATULATIONS! YOU HAVE TRAINED A BINARY CLASSIFICATION NETWORK!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
